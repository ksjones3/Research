{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa37892",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particle:\n",
    "    def __init__(self, particleText):\n",
    "        \n",
    "        #Get our list of quantities for the particle\n",
    "        particleInfo = particleText.split(\",\")\n",
    "        \n",
    "        #Define our particle quantities\n",
    "        self.obj = particleInfo[0]\n",
    "        self.E = float(particleInfo[1])\n",
    "        self.pt = float(particleInfo[2])\n",
    "        self.eta = float(particleInfo[3])\n",
    "        self.phi = float(particleInfo[4])\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"(obj: \" + self.obj + \", E: \" + str(self.E) + \", pt: \" + str(self.pt) + \", eta: \" + str(self.eta) + \", phi: \" +str(self.phi) + \")\"\n",
    "    \n",
    "    def getQuantity(self, quantityType):\n",
    "        \n",
    "        if quantityType == \"E\":\n",
    "            return self.E\n",
    "        \n",
    "        elif quantityType == \"pt\":\n",
    "            return self.pt\n",
    "        \n",
    "        elif quantityType == \"eta\":\n",
    "            return self.eta\n",
    "        \n",
    "        elif quantityType == \"phi\":\n",
    "            return self.phi\n",
    "\n",
    "        else:\n",
    "            print(\"Error: Invalid quantity type\")\n",
    "            return 0.0\n",
    "        \n",
    "    __repr__=__str__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a1783a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.utils import plot_model\n",
    "from sklearn.preprocessing import scale, normalize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.callbacks import EarlyStopping\n",
    "from random import shuffle\n",
    "from sklearn.metrics import roc_curve, auc, mean_squared_error\n",
    "from keras.losses import KLDivergence as KL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a06b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkJets(obj):\n",
    "    return obj in [\"j\", \"b\"]\n",
    "\n",
    "def checkLeptons(obj):\n",
    "    return obj in [\"e-\", \"e+\", \"m-\", \"m+\"]\n",
    "\n",
    "def checkPhotons(obj):\n",
    "    return obj==\"g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72cf43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeParticleList(eventInfo):\n",
    "    \n",
    "    #total number of datapoints in the line\n",
    "    length = len(eventInfo)\n",
    "    \n",
    "    particleList = []\n",
    "    \n",
    "    #Increment over all of the particles in the line\n",
    "    for i in range(5, length):\n",
    "        \n",
    "        #Get the text for the particle\n",
    "        particleText = eventInfo[i]\n",
    "        \n",
    "        #Make sure that it is actually information for the particle\n",
    "        if particleText != \"\" and particleText != \"\\n\":\n",
    "            \n",
    "            particle = Particle(particleText)\n",
    "            \n",
    "            particleList.append(particle)\n",
    "    \n",
    "    return particleList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff67730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeEvent(line, signal):\n",
    "    \n",
    "    eventInfo = line.split(\";\")\n",
    "    \n",
    "    eventID = eventInfo[0]\n",
    "    processID = eventInfo[1]\n",
    "    eventWeight = float(eventInfo[2])\n",
    "    MET = float(eventInfo[3])\n",
    "    METPhi = float(eventInfo[4])\n",
    "    \n",
    "    signal = float(signal)\n",
    "    \n",
    "    particleList = makeParticleList(eventInfo)\n",
    "    \n",
    "    crossSection = 1.0\n",
    "    \n",
    "    event = {\n",
    "        \"eventID\" : eventID,\n",
    "        \"processID\" : processID,\n",
    "        \"eventWeight\" : eventWeight,\n",
    "        \"MET\" : MET,\n",
    "        \"METPhi\" : METPhi,\n",
    "        \"particleList\" : particleList,\n",
    "        \"crossSection\" : crossSection,\n",
    "        \"signal\" : signal\n",
    "    }\n",
    "    \n",
    "    return event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be29f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateSignalCrossSection(event, length):\n",
    "    \n",
    "    eventWeight = event[\"eventWeight\"]\n",
    "    \n",
    "    crossSection = eventWeight*length\n",
    "    \n",
    "    return crossSection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07452df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateBackgroundCrossSection(length, luminosity):\n",
    "    \n",
    "    crossSection = length/luminosity\n",
    "    \n",
    "    return crossSection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73cb1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateCrossSection(event, length, luminosity, signal):\n",
    "    if signal:\n",
    "        return calculateSignalCrossSection(event, length)\n",
    "    else:\n",
    "        return calculateBackgroundCrossSection(length, luminosity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3faef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDataList(filePath, signal, luminosity = 1.0):\n",
    "    \n",
    "    file = open(filePath, \"r\")\n",
    "    \n",
    "    dataList = []\n",
    "    \n",
    "    for line in file:\n",
    "        \n",
    "        event = makeEvent(line, signal)\n",
    "        \n",
    "        dataList.append(event)\n",
    "\n",
    "    file.close()\n",
    "    \n",
    "    length = float(len(dataList))\n",
    "    \n",
    "    for event in dataList:\n",
    "        \n",
    "        event[\"crossSection\"] = calculateCrossSection(event, length, luminosity, signal)\n",
    "    \n",
    "    \n",
    "    return dataList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461f0795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeParticleVectors(event):\n",
    "    \n",
    "    particleVectors = []\n",
    "    particleList = event[\"particleList\"]\n",
    "    length = len(particleList)\n",
    "    \n",
    "    #jets = []\n",
    "    #leptons = []\n",
    "    \n",
    "    #for particle in particleList:\n",
    "        #if checkJets(particle.obj):\n",
    "            #jets.append(particle)\n",
    "        #elif checkLeptons(particle.obj):\n",
    "            #leptons.append(particle)\n",
    "    \n",
    "    #jetNumber = len(jets)\n",
    "    #leptonNumber = len(leptons)\n",
    "    \n",
    "    #for i in range(0, 6):\n",
    "        #if i < jetNumber:\n",
    "            #jet = jets[i]\n",
    "            #particleVectors.append(jet.pt)\n",
    "            #particleVectors.append(jet.eta)\n",
    "            #particleVectors.append(jet.phi)\n",
    "            #particleVectors.append(jet.E)\n",
    "        #else:\n",
    "            #for j in range(0, 4):\n",
    "                #particleVectors.append(0.0)\n",
    "    \n",
    "    #for i in range(0, 2):\n",
    "        #if i < leptonNumber:\n",
    "            #lepton = leptons[i]\n",
    "            #particleVectors.append(lepton.pt)\n",
    "            #particleVectors.append(lepton.eta)\n",
    "            #particleVectors.append(lepton.phi)\n",
    "            #particleVectors.append(lepton.E)\n",
    "        #else:\n",
    "            #for j in range(0, 4):\n",
    "                #particleVectors.append(0.0)\n",
    "        \n",
    "    for i in range(0, 8):\n",
    "        if i < length:\n",
    "            particle = particleList[i]\n",
    "            particleVectors.append(particle.pt)\n",
    "            particleVectors.append(particle.eta)\n",
    "            particleVectors.append(particle.phi)\n",
    "            particleVectors.append(particle.E)\n",
    "        else:\n",
    "            for n in range(0, 4):\n",
    "                particleVectors.append(0.0)\n",
    "    \n",
    "    return particleVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abfff72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleData(data):\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaledData = scaler.fit_transform(data)\n",
    "    return scaledData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ac8ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatAutoencoderData(dataList):\n",
    "    \n",
    "    autoencoderData = []\n",
    "    \n",
    "    for event in dataList:\n",
    "        particleVectors = makeParticleVectors(event)\n",
    "        \n",
    "        #tensor = tf.constant([particleVectors[0], particleVectors[1], particleVectors[2], particleVectors[3], particleVectors[4], particleVectors[5], particleVectors[6], particleVectors[7], event[\"MET\"], event[\"METPhi\"]])\n",
    "        \n",
    "        autoencoderData.append([particleVectors[0], particleVectors[1], particleVectors[2], particleVectors[3], particleVectors[4], particleVectors[5], particleVectors[6], particleVectors[7], particleVectors[8], particleVectors[9], particleVectors[10], particleVectors[11], particleVectors[12], particleVectors[13], particleVectors[14], particleVectors[15], particleVectors[16], particleVectors[17], particleVectors[18], particleVectors[19], particleVectors[20], particleVectors[21], particleVectors[22], particleVectors[23], particleVectors[24], particleVectors[25], particleVectors[26], particleVectors[27], particleVectors[28], particleVectors[29], particleVectors[30], particleVectors[31], event[\"MET\"], event[\"METPhi\"], event[\"signal\"]])\n",
    "    \n",
    "    scaledData = scaleData(autoencoderData)\n",
    "    print(scaledData)\n",
    "    \n",
    "    autoencoderDataframe = pd.DataFrame(data = scaledData, columns = [\"Particle1PT\", \"Particle1Eta\", \"Particle1Phi\", \"Particle1E\", \"Particle2PT\", \"Particle2Eta\", \"Particle2Phi\", \"Particle2E\", \"Particle3PT\", \"Particle3Eta\", \"Particle3Phi\", \"Particle3E\", \"Particle4PT\", \"Particle4Eta\", \"Particle4Phi\", \"Particle4E\", \"Particle5PT\", \"Particle5Eta\", \"Particle5Phi\", \"Particle5E\", \"Particle6PT\", \"Particle6Eta\", \"Particle6Phi\", \"Particle6E\", \"Particle7PT\", \"Particle7Eta\", \"Particle7Phi\", \"Particle7E\", \"Particle8PT\", \"Particle8Eta\", \"Particle8Phi\", \"Particle8E\", \"MET\", \"METPhi\", \"Signal\"])\n",
    "    \n",
    "    return autoencoderDataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355a0d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAutoencoderError(X_Test, autoencoder):\n",
    "    \n",
    "    distortedData = autoencoder.predict(X_Test)\n",
    "    \n",
    "    X_Test = X_Test.to_numpy()\n",
    "    y_predict = []\n",
    "    \n",
    "    for i in range(0, len(distortedData)):\n",
    "        testEvent = X_Test[i]\n",
    "        distortedEvent = distortedData[i]\n",
    "        predict = 0.0\n",
    "        for j in range(0, len(distortedEvent)):\n",
    "            predict += (distortedEvent[j]-testEvent[j])**2\n",
    "        y_predict.append(predict)\n",
    "    \n",
    "    #y_predict = scaleData(np.reshape(y_predict, (-1, 1)))\n",
    "    \n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8e49fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeArea(hist, bin_edges):\n",
    "    \n",
    "    bin_size = bin_edges[1] - bin_edges[0]\n",
    "    integral = sum(hist) * bin_size\n",
    "    normalizedCount = (1.0/integral) * hist\n",
    "    \n",
    "    return normalizedCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5feeba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHistogram(y_Predict, x_Predict, numBins):\n",
    "    y_hist, bin_edges = np.histogram(y_Predict, bins=numBins)\n",
    "    normalizedYCount = normalizeArea(y_hist, bin_edges)\n",
    "    plt.step(bin_edges[1:], normalizedYCount, label=\"Signal\")\n",
    "    x_hist, bin_edges = np.histogram(x_Predict, bins=numBins)\n",
    "    normalizedXCount = normalizeArea(x_hist, bin_edges)\n",
    "    plt.step(bin_edges[1:], normalizedXCount, label=\"Background\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46f840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_Functions:\n",
    "    def __init__(self, beta):\n",
    "        self.beta = beta\n",
    "    \n",
    "    def custom_loss_function(y_true, y_pred):\n",
    "        return (1.0-self.beta)*mean_squared_error(y_true, y_pred) + self.beta*KL(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c10a454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim),\n",
    "                              mean=0., stddev=0.1)\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "def makeVAE(beta, intermediate_dim, anomalyScore, data):\n",
    "    \n",
    "    original_dim = data.shape[1]\n",
    "    latent_dim = 2\n",
    "\n",
    "    inputs = Input(shape=(original_dim,))\n",
    "    h = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "    z_mean = Dense(latent_dim)(h)\n",
    "    z_log_sigma = Dense(latent_dim)(h)\n",
    "    \n",
    "    z = layers.Lambda(sampling)([z_mean, z_log_sigma])\n",
    "    \n",
    "    # Create encoder\n",
    "    encoder = keras.Model(inputs, [z_mean, z_log_sigma, z], name='encoder')\n",
    "\n",
    "    # Create decoder\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,), name='z_sampling')\n",
    "    x = layers.Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "    outputs = layers.Dense(original_dim, activation='sigmoid')(x)\n",
    "    decoder = keras.Model(latent_inputs, outputs, name='decoder')\n",
    "\n",
    "    # instantiate VAE model\n",
    "    outputs = decoder(encoder(inputs)[2])\n",
    "    vae = keras.Model(inputs, outputs, name='vae_mlp')\n",
    "    \n",
    "    functions = LossFunctions(beta)\n",
    "    custom_Loss_Function = functions.custom_loss_function\n",
    "    \n",
    "    vae.compile(optimizer = \"adam\", loss = custom_loss_function)\n",
    "    \n",
    "    return vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8b5c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "backgroundDataList = makeDataList(\"Data/training_files/training_files/chan1/background_chan1_7.79.csv\", False, luminosity = 7.79)\n",
    "signalDataList = makeDataList(\"Data/training_files/training_files/chan1/glgl1400_neutralino1100_chan1.csv\", True)\n",
    "length = len(signalDataList)\n",
    "testDataList = signalDataList+backgroundDataList[:length]\n",
    "trainingDataList = backgroundDataList[length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1daba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoderBackgroundData = formatAutoencoderData(trainingDataList)\n",
    "autoencoderTestData = formatAutoencoderData(testDataList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fddee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "VarNames = [\"Particle1PT\", \"Particle1Eta\", \"Particle1Phi\", \"Particle1E\", \"Particle2PT\", \"Particle2Eta\", \"Particle2Phi\", \"Particle2E\", \"Particle3PT\", \"Particle3Eta\", \"Particle3Phi\", \"Particle3E\", \"Particle4PT\", \"Particle4Eta\", \"Particle4Phi\", \"Particle4E\", \"Particle5PT\", \"Particle5Eta\", \"Particle5Phi\", \"Particle5E\", \"Particle6PT\", \"Particle6Eta\", \"Particle6Phi\", \"Particle6E\", \"Particle7PT\", \"Particle7Eta\", \"Particle7Phi\", \"Particle7E\", \"Particle8PT\", \"Particle8Eta\", \"Particle8Phi\", \"Particle8E\", \"MET\", \"METPhi\"]\n",
    "autoencoderBackgroundData = autoencoderBackgroundData[VarNames]\n",
    "X_Test = autoencoderTestData[VarNames]\n",
    "y_Test = autoencoderTestData[\"Signal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b71608",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim=autoencoderBackgroundData.shape[1]\n",
    "\n",
    "input_vec = Input(shape =(input_dim,))\n",
    "\n",
    "encoded = Dense(34, activation='relu')(input_vec)\n",
    "encoded = Dense(200, activation='tanh')(encoded)\n",
    "encoded = Dense(200, activation='tanh')(encoded)\n",
    "encoded = Dense(20, activation='tanh')(encoded)\n",
    "mean = Dense(34, )\n",
    "decoded = Dense(10, activation='tanh')(encoded)\n",
    "decoded = Dense(20, activation='tanh')(decoded)\n",
    "decoded = Dense(200, activation='tanh')(decoded)\n",
    "decoded = Dense(200, activation='tanh')(decoded)\n",
    "decoded = Dense(input_dim, activation='relu')(decoded)\n",
    "\n",
    "autoencoder=Model(input_vec, decoded)\n",
    "\n",
    "\n",
    "autoencoder.compile(optimizer=\"adam\",\n",
    "                    loss=\"mean_squared_error\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b802f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.00005, patience=20)\n",
    "\n",
    "history=autoencoder.fit(autoencoderBackgroundData, autoencoderBackgroundData, epochs=100,\n",
    "               batch_size=125,\n",
    "               shuffle='batch',\n",
    "               validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fa275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_Predict = calculateAutoencoderError(X_Test, autoencoder)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_Test, y_Predict)\n",
    "                        \n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr,tpr,color='darkorange',label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b508679",
   "metadata": {},
   "outputs": [],
   "source": [
    "backgroundAutoencoderData = formatAutoencoderData(backgroundDataList)[VarNames]\n",
    "signalAutoencoderData = formatAutoencoderData(signalDataList)[VarNames]\n",
    "x_Predict = calculateAutoencoderError(backgroundAutoencoderData, autoencoder)\n",
    "y_Predict = calculateAutoencoderError(signalAutoencoderData, autoencoder)\n",
    "\n",
    "plotHistogram(y_Predict, x_Predict, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e26608",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(history.history[\"loss\"])),history.history[\"loss\"],label=\"Training Loss\")\n",
    "plt.plot(range(len(history.history[\"val_loss\"])),history.history[\"val_loss\"],label=\"Validation Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c77ea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "distortedData = autoencoder.predict(X_Test)\n",
    "y_Predict = []\n",
    "X_Test_numpy = X_Test.to_numpy()\n",
    "for i in range(0, len(distortedData)):\n",
    "    X = X_Test_numpy[i]\n",
    "    Y = distortedData[i]\n",
    "    predict = mean_squared_error(X, Y)\n",
    "    y_Predict.append(predict)\n",
    "    \n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_Test, y_Predict)\n",
    "                        \n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr,tpr,color='darkorange',label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ebf46e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
